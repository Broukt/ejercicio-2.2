{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea97f271",
   "metadata": {},
   "source": [
    "## Ejercicio 2.2: Modelo de regresión logística que predice si un individuo tendrá una cita en el día de San Valentín o no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ef9ab",
   "metadata": {},
   "source": [
    "**Enunciado**\n",
    "\n",
    "Se tiene el siguiente conjunto de datos sintéticos creado para permitir a los aspirantes a científicos o analistas de datos llevar a cabo expreimentos predictivos mediante la construcción de un modelo de aprendizaje automático que puede predecir si un individuo obtendrá una cita de San Valentín o no.\n",
    "\n",
    "El conjunto de datos tiene las siguientes características:\n",
    "\n",
    "- Name: el nombre ficticio de una persona.\n",
    "- Age: la edad ficticia de una persona.\n",
    "- Gender: el género de  una persona.\n",
    "- Income: lo que la persona gana anualmente como ingreso.\n",
    "- Appearance Score: esta característica representa una puntuación numérica que indica cómo se percibe a los individuos en función de su apariencia física.\n",
    "- Interest Score: esta característica representa una puntuación numérica que indica qué tan alineados o compatibles están los intereses de una persona con los de los demás.\n",
    "- Confidence Score: esta característica representa una puntuación numérica que indica el nivel de confianza percibido de un individuo.\n",
    "- Educational Status: esta característica representa el logro educativo o el nivel de educación de un individuo.\n",
    "- Job Type: esta característica representa el tipo de trabajo o situación laboral de un individuo.\n",
    "- Valentine Date: esta es una característica binaria utilizada para la clasificación binaria (0, 1), que indica si un individuo tiene una fecha de San Valentín (1) o no (0).\n",
    "\n",
    "El conjunto de datos se encuentra en el archivo: ***ValentineDataset.csv***.\n",
    "\n",
    "Utilizando un conjunto de datos proporcionado, realizar lo siguiente:\n",
    "\n",
    "<ol type=\"a\">\n",
    "<li>Cargar y explorar los datos.</li>\n",
    "<li>Hallar la matriz de correlación de los datos para determinar las variables más significativas a usar en el modelo.</li>\n",
    "<li>Dividir los datos en 90% para entrenamiento y el resto para prueba.</li>\n",
    "<li>Implementar un modelo de regresión logística utilizando Python y entrenarlo.</li>\n",
    "<li>Graficar la función de pérdida obtenida durante entrenamiento y analizarla.</li>\n",
    "<li>Evaluar el modelo utilizando la matriz de confusión y las métricas: precision, accuracy y recall.</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779df5b",
   "metadata": {},
   "source": [
    "### Importar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165b147",
   "metadata": {},
   "source": [
    "### Cargar y explorar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ValentineDataset.csv')\n",
    "df = df.dropna()\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5645df",
   "metadata": {},
   "source": [
    "### Hallar la matriz de correlación de los datos para determinar las variables más significativas a usar en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan columnas innecesarias o que no aportan información relevante\n",
    "df = df.drop(columns=['Name'])\n",
    "\n",
    "# One-hot encoding para variables categóricas\n",
    "cat_cols = ['Gender', 'Educational_Status', 'Job_Type']\n",
    "df_enc = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Se obtiene la matriz de correlación\n",
    "corr_matrix = df_enc.corr()\n",
    "\n",
    "# Se ordenan las variables por su correlación con la variable objetivo\n",
    "corr_target = corr_matrix['Valentine_Date'].sort_values(ascending=False)\n",
    "print(\"Correlación de cada variable con Valentine Date:\\n\", corr_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066bea7",
   "metadata": {},
   "source": [
    "### División de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a95fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_enc.drop(columns=['Valentine_Date'])\n",
    "y = df_enc['Valentine_Date']\n",
    "X = X.astype(np.float64)\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "# Normalización de los datos\n",
    "X = (X - X.mean()) / X.std()\n",
    "# Se dividen los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1afe1",
   "metadata": {},
   "source": [
    "### Funciones para el modelo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9035ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def loss(h, y):\n",
    "    return -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, alpha = 0.05, num_iterations = 1000):\n",
    "    m = X.shape[0]\n",
    "    weights = np.random.rand(X.shape[1])\n",
    "    loss_values = []\n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(X, weights)\n",
    "        sigmoid_values = sigmoid(z)\n",
    "        gradient = np.dot(X.T, (sigmoid_values - y)) / m\n",
    "        weights = weights - alpha * gradient\n",
    "        current_loss = loss(sigmoid_values, y)\n",
    "        loss_values.append(current_loss)\n",
    "        if i % 100 == 0:\n",
    "            print(\"Interacción:\", i, \"Pérdida:\", current_loss)\n",
    "    return weights, loss_values\n",
    "\n",
    "\n",
    "def predict(X, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    return np.where(sigmoid(z) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ef6eb",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, loss_values = gradient_descent(X_train, y_train, alpha=0.005, num_iterations=100000)\n",
    "y_pred = predict(X_test, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f5750",
   "metadata": {},
   "source": [
    "### Gráfico de la función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb841564",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loss_values, bins=50)\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
